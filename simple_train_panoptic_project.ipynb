{"cells":[{"cell_type":"markdown","source":["This notebook was used in order to try and train the OASIS model on panoptic images."],"metadata":{"id":"AjsTzOoingRW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1-LNCK2jx-3"},"outputs":[],"source":["#need to run at the start of every instance and restart kernerl\n","! pip install torchinfo\n","! pip install cityscapesscripts\n","! pip install --upgrade scipy\n","! pip install ipywidgets --user\n","! pip install nbconvert"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dYQY7sA5jx--","outputId":"0320cced-f27d-4e63-e2d4-2b4674948051"},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'OASIS' already exists and is not an empty directory.\n","fatal: destination path 'deeplab2' already exists and is not an empty directory.\n"]}],"source":["! git clone \"https://github.com/ItaiBear/OASIS\"\n","! git clone \"https://github.com/google-research/deeplab2.git\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8-vKv2Wyjx_A","outputId":"615beb0d-3cf5-4576-dc67-78393be4c238"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/ubuntu/bearGAN\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1yKpNUqGjx_C","outputId":"0d7f929c-5ad1-4610-968c-36b4fb08796f"},"outputs":[{"name":"stdout","output_type":"stream","text":["['/home/ubuntu/bearGAN', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '', '/home/ubuntu/.local/lib/python3.8/site-packages', '/usr/local/lib/python3.8/dist-packages', '/usr/lib/python3/dist-packages', '/usr/lib/python3/dist-packages/IPython/extensions', '/home/ubuntu/.ipython', '/home/ubuntu/bearGAN', '/home/ubuntu/bearGAN/OASIS', '/home/ubuntu/bearGAN/OASIS/dataloaders', '/home/ubuntu/bearGAN/OASIS/models', '/home/ubuntu/bearGAN/OASIS/utils', '/home/ubuntu/bearGAN/OASIS/models/sync_batchnorm']\n"]}],"source":["import numpy as np\n","import random\n","import torch\n","from torchvision import transforms as TR\n","from torchinfo import summary\n","import matplotlib.pyplot as plt\n","from matplotlib import gridspec\n","from PIL import Image\n","from types import SimpleNamespace\n","import os\n","import sys\n","import scipy\n","from scipy import linalg\n","\n","sys.path.append(\"/home/ubuntu/bearGAN\")\n","sys.path.append(\"/home/ubuntu/bearGAN/OASIS\")\n","sys.path.append(\"/home/ubuntu/bearGAN/OASIS/dataloaders\")\n","sys.path.append(\"/home/ubuntu/bearGAN/OASIS/models\")\n","sys.path.append(\"/home/ubuntu/bearGAN/OASIS/utils\")\n","sys.path.append(\"/home/ubuntu/bearGAN/OASIS/models/sync_batchnorm\")\n","\n","\n","\n","#sys.path.append(os.path.abspath(\"/content/BearGAN\"))\n","#sys.path.append(os.path.abspath(\"/content/deeplab2\"))\n","print(sys.path)\n","import OASIS\n","#import BearGAN\n","\n","\n","import models.models as models\n","import models.losses as losses\n","import dataloaders.dataloaders as dataloaders\n","import utils.utils as utils\n","from utils.fid_scores import fid_pytorch\n","import config\n","from constants import root_project_directory\n","\n","from my_dataloaders import get_dataloaders\n","from my_utils import configure_arguments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hzcvGU_Kjx_E","outputId":"fdfab859-fa31-4c87-ee5c-70573ad80fac"},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n","0\n","None\n"]}],"source":["print(torch.cuda.device_count())\n","import gc\n","print(gc.collect())\n","print(torch.cuda.empty_cache())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQomZFWUjx_F"},"outputs":[],"source":["def preprocess_input(opt, data):\n","    data['label'] = data['label'].long()\n","    if opt.gpu_ids != \"-1\":\n","        data['label'] = data['label'].cuda()\n","        data['image'] = data['image'].cuda()\n","    label_map = data['label']\n","    bs, _, h, w = label_map.size()\n","    nc = opt.semantic_nc\n","    if opt.gpu_ids != \"-1\":\n","        input_label = torch.cuda.FloatTensor(bs, nc, h, w).zero_()\n","    else:\n","        input_label = torch.FloatTensor(bs, nc, h, w).zero_()\n","    \n","    if (opt.segmentation == \"panoptic\"):\n","      semantic_map = torch.div(label_map, 1000, rounding_mode=\"floor\")\n","      semantic_map = semantic_map.masked_fill_(semantic_map==255, 19)\n","      instance_map = torch.fmod(label_map, 1000.0) + 1.0\n","      input_semantics = input_label.scatter_(1, semantic_map, instance_map)\n","    else:\n","      input_semantics = input_label.scatter_(1, label_map, 1.0)\n","    \n","    return data['image'], input_semantics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Hv6uwhvjx_G","outputId":"e8ff13d9-bb46-4665-dbc2-4a622c5f48fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------- Options ---------------\n","                EMA_decay: 0.999                         \t[default: 0.9999]\n","             add_vgg_loss: False                         \n","               batch_size: 14                            \t[default: 20]\n","                    beta1: 0.0                           \n","                    beta2: 0.999                         \n","               channels_D: 64                            \n","               channels_G: 64                            \n","          checkpoints_dir: /home/ubuntu/bearGAN/pretrained_checkpoints\n","           continue_train: False                         \t[default: True]\n","                 dataroot: /home/ubuntu/bearGAN/dataset  \n","             dataset_mode: cityscapes                    \n","                 freq_fid: 100000                        \t[default: 5000]\n","               freq_print: 100                           \t[default: 1000]\n","           freq_save_ckpt: 500                           \t[default: 20000]\n","         freq_save_latest: 500                           \t[default: 10000]\n","           freq_save_loss: 500                           \t[default: 2500]\n","         freq_smooth_loss: 100                           \t[default: 250]\n","                  gpu_ids: 0                             \n","          lambda_labelmix: 5.0                           \t[default: 10.0]\n","               lambda_vgg: 10.0                          \n","       loaded_latest_iter: None                          \n","                     lr_d: 0.0004                        \n","                     lr_g: 0.0004                        \t[default: 0.0001]\n","                     name: my_cityscapes_cool_panoptic   \t[default: oasis_cityscapes_pretrained]\n","               no_3dnoise: True                          \t[default: False]\n","                   no_EMA: True                          \t[default: False]\n","      no_balancing_inloss: False                         \n","                  no_flip: False                         \n","              no_labelmix: False                         \n","         no_spectral_norm: False                         \n","               num_epochs: 200                           \t[default: 10]\n","           num_res_blocks: 6                             \n","          param_free_norm: syncbatch                     \n","                    phase: train                         \n","                     seed: 42                            \n","             segmentation: panoptic                      \t[default: semantic]\n","                 spade_ks: 3                             \n","               which_iter: latest                        \n","                    z_dim: 64                            \n","----------------- End -------------------\n","configured arguments\n","getting panoptic datasets\n","Created CityscapesDataset, size train: 2975, size val: 500\n"]},{"name":"stderr","output_type":"stream","text":["/usr/lib/python3/dist-packages/torchvision/models/inception.py:81: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n","  warnings.warn('The default weight initialization of inception_v3 will be changed in future releases of '\n"]},{"name":"stdout","output_type":"stream","text":["--- Now computing Inception activations for real set ---\n"]},{"name":"stderr","output_type":"stream","text":["/usr/lib/python3/dist-packages/torchvision/transforms/functional.py:404: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["--- Finished FID stats for real set ---\n","creating models\n","Created OASIS_Generator with 68923331 parameters\n","Created OASIS_Discriminator with 22250389 parameters\n"]},{"name":"stderr","output_type":"stream","text":["/usr/lib/python3/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"]},{"name":"stdout","output_type":"stream","text":["[epoch 0/200 - iter 0], time:0.000\n","[epoch 0/200 - iter 100], time:6.748\n","[epoch 0/200 - iter 200], time:6.768\n","[epoch 1/200 - iter 300], time:6.722\n","[epoch 1/200 - iter 400], time:6.697\n","[epoch 2/200 - iter 500], time:6.732\n","[epoch 2/200 - iter 600], time:6.714\n","[epoch 3/200 - iter 700], time:6.703\n","[epoch 3/200 - iter 800], time:6.699\n","[epoch 4/200 - iter 900], time:6.723\n","[epoch 4/200 - iter 1000], time:6.726\n","[epoch 5/200 - iter 1100], time:6.728\n","[epoch 5/200 - iter 1200], time:6.710\n","[epoch 6/200 - iter 1300], time:6.708\n","[epoch 6/200 - iter 1400], time:6.725\n","[epoch 7/200 - iter 1500], time:6.739\n","[epoch 7/200 - iter 1600], time:6.808\n","[epoch 8/200 - iter 1700], time:6.699\n","[epoch 8/200 - iter 1800], time:6.708\n","[epoch 8/200 - iter 1900], time:6.710\n","[epoch 9/200 - iter 2000], time:6.758\n","[epoch 9/200 - iter 2100], time:6.711\n","[epoch 10/200 - iter 2200], time:6.726\n","[epoch 10/200 - iter 2300], time:6.682\n","[epoch 11/200 - iter 2400], time:6.696\n","[epoch 11/200 - iter 2500], time:6.741\n","[epoch 12/200 - iter 2600], time:6.726\n","[epoch 12/200 - iter 2700], time:6.688\n","[epoch 13/200 - iter 2800], time:6.725\n","[epoch 13/200 - iter 2900], time:6.711\n","[epoch 14/200 - iter 3000], time:6.752\n","[epoch 14/200 - iter 3100], time:6.733\n","[epoch 15/200 - iter 3200], time:6.716\n","[epoch 15/200 - iter 3300], time:6.705\n","[epoch 16/200 - iter 3400], time:6.671\n","[epoch 16/200 - iter 3500], time:6.762\n","[epoch 16/200 - iter 3600], time:6.703\n","[epoch 17/200 - iter 3700], time:6.648\n","[epoch 17/200 - iter 3800], time:6.748\n","[epoch 18/200 - iter 3900], time:6.702\n","[epoch 18/200 - iter 4000], time:6.759\n","[epoch 19/200 - iter 4100], time:6.791\n","[epoch 19/200 - iter 4200], time:6.732\n","[epoch 20/200 - iter 4300], time:6.733\n","[epoch 20/200 - iter 4400], time:6.774\n","[epoch 21/200 - iter 4500], time:6.784\n","[epoch 21/200 - iter 4600], time:6.751\n","[epoch 22/200 - iter 4700], time:6.742\n","[epoch 22/200 - iter 4800], time:6.738\n","[epoch 23/200 - iter 4900], time:6.737\n","[epoch 23/200 - iter 5000], time:6.777\n","[epoch 24/200 - iter 5100], time:6.763\n","[epoch 24/200 - iter 5200], time:6.741\n","[epoch 25/200 - iter 5300], time:6.717\n","[epoch 25/200 - iter 5400], time:6.714\n","[epoch 25/200 - iter 5500], time:6.790\n","[epoch 26/200 - iter 5600], time:6.756\n","[epoch 26/200 - iter 5700], time:6.768\n","[epoch 27/200 - iter 5800], time:6.749\n"]}],"source":["#--- read options ---#\n","opt = configure_arguments(train=True)\n","print(\"configured arguments\")\n","#--- create utils ---#\n","timer = utils.timer(opt)\n","visualizer_losses = utils.losses_saver(opt)\n","losses_computer = losses.losses_computer(opt)\n","dataloader, dataloader_val = get_dataloaders(opt)\n","im_saver = utils.image_saver(opt)\n","fid_computer = fid_pytorch(opt, dataloader_val)   #problem with tpus\n","\n","#--- create models ---#\n","print(\"creating models\")\n","model = models.OASIS_model(opt)\n","model = models.put_on_multi_gpus(model, opt)\n","\n","#--- create optimizers ---#\n","optimizerG = torch.optim.Adam(model.module.netG.parameters(), lr=opt.lr_g, betas=(opt.beta1, opt.beta2))\n","optimizerD = torch.optim.Adam(model.module.netD.parameters(), lr=opt.lr_d, betas=(opt.beta1, opt.beta2))\n","\n","#--- the training loop ---#\n","already_started = False\n","start_epoch, start_iter = utils.get_start_iters(opt.loaded_latest_iter, len(dataloader))\n","for epoch in range(start_epoch, opt.num_epochs):\n","    for i, data_i in enumerate(dataloader):\n","        if not already_started and i < start_iter:\n","            continue\n","        already_started = True\n","        cur_iter = epoch*len(dataloader) + i\n","        image, label = preprocess_input(opt, data_i)\n","\n","        #--- generator update ---#\n","        model.module.netG.zero_grad()\n","        loss_G, losses_G_list = model.forward(image, label, \"losses_G\", losses_computer)\n","        loss_G, losses_G_list = loss_G.mean(), [loss.mean() if loss is not None else None for loss in losses_G_list]\n","        loss_G.backward()\n","        optimizerG.step()\n","\n","        #--- discriminator update ---#\n","        model.module.netD.zero_grad()\n","        loss_D, losses_D_list = model.forward(image, label, \"losses_D\", losses_computer)\n","        loss_D, losses_D_list = loss_D.mean(), [loss.mean() if loss is not None else None for loss in losses_D_list]\n","        loss_D.backward()\n","        optimizerD.step()\n","\n","        #--- stats update ---#\n","        if not opt.no_EMA:\n","            utils.update_EMA(model, cur_iter, dataloader, opt)\n","        if cur_iter % opt.freq_print == 0:\n","            im_saver.visualize_batch(model, image, label, cur_iter)\n","            timer(epoch, cur_iter)\n","        if cur_iter % opt.freq_save_ckpt == 0:\n","            utils.save_networks(opt, cur_iter, model)\n","        if cur_iter % opt.freq_save_latest == 0:\n","            utils.save_networks(opt, cur_iter, model, latest=True)\n","        if cur_iter % opt.freq_fid == 0 and cur_iter > 0:\n","            is_best = fid_computer.update(model, cur_iter)\n","            if is_best:\n","                utils.save_networks(opt, cur_iter, model, best=True)\n","        visualizer_losses(cur_iter, losses_G_list+losses_D_list)\n","\n","#--- after training ---#\n","#utils.update_EMA(model, cur_iter, dataloader, opt, force_run_stats=True)\n","utils.save_networks(opt, cur_iter, model)\n","utils.save_networks(opt, cur_iter, model, latest=True)\n","is_best = fid_computer.update(model, cur_iter)\n","if is_best:\n","    utils.save_networks(opt, cur_iter, model, best=True)\n","\n","print(\"The training has successfully finished\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mTF-aNWbjx_J"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q57SlE60jx_J"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"toc-showcode":true,"toc-showtags":false,"colab":{"name":"simple_train_panoptic_project.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}